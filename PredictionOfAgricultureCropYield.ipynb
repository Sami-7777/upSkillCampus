{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a92a9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found CSV files: ['pesticides.csv', 'rainfall.csv', 'temp.csv', 'yield.csv', 'yield_df.csv']\n",
      "\n",
      "============================================================\n",
      "File: pesticides.csv\n",
      "- shape: (4349, 7)\n",
      "\n",
      "-- head()\n",
      "        Domain    Area Element               Item  Year                         Unit  Value\n",
      "Pesticides Use Albania     Use Pesticides (total)  1990 tonnes of active ingredients  121.0\n",
      "Pesticides Use Albania     Use Pesticides (total)  1991 tonnes of active ingredients  121.0\n",
      "Pesticides Use Albania     Use Pesticides (total)  1992 tonnes of active ingredients  121.0\n",
      "Pesticides Use Albania     Use Pesticides (total)  1993 tonnes of active ingredients  121.0\n",
      "Pesticides Use Albania     Use Pesticides (total)  1994 tonnes of active ingredients  201.0\n",
      "\n",
      "-- dtypes:\n",
      "Domain      object\n",
      "Area        object\n",
      "Element     object\n",
      "Item        object\n",
      "Year         int64\n",
      "Unit        object\n",
      "Value      float64\n",
      "dtype: object\n",
      "\n",
      "-- describe():\n",
      "        count          mean            std     min     25%      50%     75%  \\\n",
      "Year   4349.0   2003.138883       7.728044  1990.0  1996.0  2003.00  2010.0   \n",
      "Value  4349.0  20303.341465  117736.190448     0.0    93.0  1137.56  7869.0   \n",
      "\n",
      "             max  \n",
      "Year      2016.0  \n",
      "Value  1807000.0  \n",
      "\n",
      "-- top missing columns:\n",
      "         missing_count  missing_pct\n",
      "Domain               0          0.0\n",
      "Area                 0          0.0\n",
      "Element              0          0.0\n",
      "Item                 0          0.0\n",
      "Year                 0          0.0\n",
      "Unit                 0          0.0\n",
      "Value                0          0.0\n",
      "\n",
      "============================================================\n",
      "File: rainfall.csv\n",
      "- shape: (6727, 3)\n",
      "\n",
      "-- head()\n",
      "       Area  Year average_rain_fall_mm_per_year\n",
      "Afghanistan  1985                           327\n",
      "Afghanistan  1986                           327\n",
      "Afghanistan  1987                           327\n",
      "Afghanistan  1989                           327\n",
      "Afghanistan  1990                           327\n",
      "\n",
      "-- dtypes:\n",
      " Area                            object\n",
      "Year                              int64\n",
      "average_rain_fall_mm_per_year    object\n",
      "dtype: object\n",
      "\n",
      "-- describe():\n",
      "       count         mean       std     min     25%     50%     75%     max\n",
      "Year  6727.0  2001.354839  9.530114  1985.0  1993.0  2001.0  2010.0  2017.0\n",
      "\n",
      "-- top missing columns:\n",
      "                               missing_count  missing_pct\n",
      "average_rain_fall_mm_per_year            774    11.505872\n",
      " Area                                      0     0.000000\n",
      "Year                                       0     0.000000\n",
      "\n",
      "============================================================\n",
      "File: temp.csv\n",
      "- shape: (71311, 3)\n",
      "\n",
      "-- head()\n",
      " year       country  avg_temp\n",
      " 1849 Côte D'Ivoire     25.58\n",
      " 1850 Côte D'Ivoire     25.52\n",
      " 1851 Côte D'Ivoire     25.67\n",
      " 1852 Côte D'Ivoire       NaN\n",
      " 1853 Côte D'Ivoire       NaN\n",
      "\n",
      "-- dtypes:\n",
      "year          int64\n",
      "country      object\n",
      "avg_temp    float64\n",
      "dtype: object\n",
      "\n",
      "-- describe():\n",
      "            count         mean        std      min      25%      50%  \\\n",
      "year      71311.0  1905.799007  67.102099  1743.00  1858.00  1910.00   \n",
      "avg_temp  68764.0    16.183876   7.592960   -14.35     9.75    16.14   \n",
      "\n",
      "                75%      max  \n",
      "year      1962.0000  2013.00  \n",
      "avg_temp    23.7625    30.73  \n",
      "\n",
      "-- top missing columns:\n",
      "          missing_count  missing_pct\n",
      "avg_temp           2547     3.571679\n",
      "year                  0     0.000000\n",
      "country               0     0.000000\n",
      "\n",
      "============================================================\n",
      "File: yield.csv\n",
      "- shape: (56717, 12)\n",
      "\n",
      "-- head()\n",
      "Domain Code Domain  Area Code        Area  Element Code Element  Item Code  Item  Year Code  Year  Unit  Value\n",
      "         QC  Crops          2 Afghanistan          5419   Yield         56 Maize       1961  1961 hg/ha  14000\n",
      "         QC  Crops          2 Afghanistan          5419   Yield         56 Maize       1962  1962 hg/ha  14000\n",
      "         QC  Crops          2 Afghanistan          5419   Yield         56 Maize       1963  1963 hg/ha  14260\n",
      "         QC  Crops          2 Afghanistan          5419   Yield         56 Maize       1964  1964 hg/ha  14257\n",
      "         QC  Crops          2 Afghanistan          5419   Yield         56 Maize       1965  1965 hg/ha  14400\n",
      "\n",
      "-- dtypes:\n",
      "Domain Code     object\n",
      "Domain          object\n",
      "Area Code        int64\n",
      "Area            object\n",
      "Element Code     int64\n",
      "Element         object\n",
      "Item Code        int64\n",
      "Item            object\n",
      "Year Code        int64\n",
      "Year             int64\n",
      "Unit            object\n",
      "Value            int64\n",
      "dtype: object\n",
      "\n",
      "-- describe():\n",
      "                count          mean           std     min      25%      50%  \\\n",
      "Area Code     56717.0    125.650422     75.120195     1.0     58.0    122.0   \n",
      "Element Code  56717.0   5419.000000      0.000000  5419.0   5419.0   5419.0   \n",
      "Item Code     56717.0    111.611651    101.278435    15.0     56.0    116.0   \n",
      "Year Code     56717.0   1989.669570     16.133198  1961.0   1976.0   1991.0   \n",
      "Year          56717.0   1989.669570     16.133198  1961.0   1976.0   1991.0   \n",
      "Value         56717.0  62094.660084  67835.932856     0.0  15680.0  36744.0   \n",
      "\n",
      "                  75%        max  \n",
      "Area Code       184.0      351.0  \n",
      "Element Code   5419.0     5419.0  \n",
      "Item Code       125.0      489.0  \n",
      "Year Code      2004.0     2016.0  \n",
      "Year           2004.0     2016.0  \n",
      "Value         86213.0  1000000.0  \n",
      "\n",
      "-- top missing columns:\n",
      "              missing_count  missing_pct\n",
      "Domain Code               0          0.0\n",
      "Domain                    0          0.0\n",
      "Area Code                 0          0.0\n",
      "Area                      0          0.0\n",
      "Element Code              0          0.0\n",
      "Element                   0          0.0\n",
      "Item Code                 0          0.0\n",
      "Item                      0          0.0\n",
      "Year Code                 0          0.0\n",
      "Year                      0          0.0\n",
      "Unit                      0          0.0\n",
      "Value                     0          0.0\n",
      "\n",
      "============================================================\n",
      "File: yield_df.csv\n",
      "- shape: (28242, 8)\n",
      "\n",
      "-- head()\n",
      " Unnamed: 0    Area        Item  Year  hg/ha_yield  average_rain_fall_mm_per_year  pesticides_tonnes  avg_temp\n",
      "          0 Albania       Maize  1990        36613                         1485.0              121.0     16.37\n",
      "          1 Albania    Potatoes  1990        66667                         1485.0              121.0     16.37\n",
      "          2 Albania Rice, paddy  1990        23333                         1485.0              121.0     16.37\n",
      "          3 Albania     Sorghum  1990        12500                         1485.0              121.0     16.37\n",
      "          4 Albania    Soybeans  1990         7000                         1485.0              121.0     16.37\n",
      "\n",
      "-- dtypes:\n",
      "Unnamed: 0                         int64\n",
      "Area                              object\n",
      "Item                              object\n",
      "Year                               int64\n",
      "hg/ha_yield                        int64\n",
      "average_rain_fall_mm_per_year    float64\n",
      "pesticides_tonnes                float64\n",
      "avg_temp                         float64\n",
      "dtype: object\n",
      "\n",
      "-- describe():\n",
      "                                 count          mean           std      min  \\\n",
      "Unnamed: 0                     28242.0  14120.500000   8152.907488     0.00   \n",
      "Year                           28242.0   2001.544296      7.051905  1990.00   \n",
      "hg/ha_yield                    28242.0  77053.332094  84956.612897    50.00   \n",
      "average_rain_fall_mm_per_year  28242.0   1149.055980    709.812150    51.00   \n",
      "pesticides_tonnes              28242.0  37076.909344  59958.784665     0.04   \n",
      "avg_temp                       28242.0     20.542627      6.312051     1.30   \n",
      "\n",
      "                                      25%       50%        75%        max  \n",
      "Unnamed: 0                      7060.2500  14120.50   21180.75   28241.00  \n",
      "Year                            1995.0000   2001.00    2008.00    2013.00  \n",
      "hg/ha_yield                    19919.2500  38295.00  104676.75  501412.00  \n",
      "average_rain_fall_mm_per_year    593.0000   1083.00    1668.00    3240.00  \n",
      "pesticides_tonnes               1702.0000  17529.44   48687.88  367778.00  \n",
      "avg_temp                          16.7025     21.51      26.00      30.65  \n",
      "\n",
      "-- top missing columns:\n",
      "                               missing_count  missing_pct\n",
      "Unnamed: 0                                 0          0.0\n",
      "Area                                       0          0.0\n",
      "Item                                       0          0.0\n",
      "Year                                       0          0.0\n",
      "hg/ha_yield                                0          0.0\n",
      "average_rain_fall_mm_per_year              0          0.0\n",
      "pesticides_tonnes                          0          0.0\n",
      "avg_temp                                   0          0.0\n",
      "\n",
      "Summary table:\n",
      "          file       shape  num_columns  num_missing_columns\n",
      "pesticides.csv   (4349, 7)            7                    0\n",
      "  rainfall.csv   (6727, 3)            3                    1\n",
      "      temp.csv  (71311, 3)            3                    1\n",
      "     yield.csv (56717, 12)           12                    0\n",
      "  yield_df.csv  (28242, 8)            8                    0\n",
      "Common columns across files: ['Domain', 'Area', 'Element', 'Item', 'Year', 'Unit', 'Value', 'average_rain_fall_mm_per_year', 'avg_temp']\n",
      "Candidate merge keys: ['Year']\n",
      "Merging on key: Year\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 7.37 GiB for an array with shape (989712018,) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 101\u001b[0m, in \u001b[0;36mattempt_merge\u001b[1;34m(dfs)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 101\u001b[0m     merged \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(merged, df, on\u001b[38;5;241m=\u001b[39mkey, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mouter\u001b[39m\u001b[38;5;124m'\u001b[39m, suffixes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;66;03m# try string conversion\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:184\u001b[0m, in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    170\u001b[0m op \u001b[38;5;241m=\u001b[39m _MergeOperation(\n\u001b[0;32m    171\u001b[0m     left_df,\n\u001b[0;32m    172\u001b[0m     right_df,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m     validate\u001b[38;5;241m=\u001b[39mvalidate,\n\u001b[0;32m    183\u001b[0m )\n\u001b[1;32m--> 184\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result(copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:888\u001b[0m, in \u001b[0;36m_MergeOperation.get_result\u001b[1;34m(self, copy)\u001b[0m\n\u001b[0;32m    886\u001b[0m join_index, left_indexer, right_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_join_info()\n\u001b[1;32m--> 888\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_and_concat(\n\u001b[0;32m    889\u001b[0m     join_index, left_indexer, right_indexer, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    890\u001b[0m )\n\u001b[0;32m    891\u001b[0m result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_type)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:848\u001b[0m, in \u001b[0;36m_MergeOperation._reindex_and_concat\u001b[1;34m(self, join_index, left_indexer, right_indexer, copy)\u001b[0m\n\u001b[0;32m    844\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m left_indexer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_range_indexer(left_indexer, \u001b[38;5;28mlen\u001b[39m(left)):\n\u001b[0;32m    845\u001b[0m     \u001b[38;5;66;03m# Pinning the index here (and in the right code just below) is not\u001b[39;00m\n\u001b[0;32m    846\u001b[0m     \u001b[38;5;66;03m#  necessary, but makes the `.take` more performant if we have e.g.\u001b[39;00m\n\u001b[0;32m    847\u001b[0m     \u001b[38;5;66;03m#  a MultiIndex for left.index.\u001b[39;00m\n\u001b[1;32m--> 848\u001b[0m     lmgr \u001b[38;5;241m=\u001b[39m left\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mreindex_indexer(\n\u001b[0;32m    849\u001b[0m         join_index,\n\u001b[0;32m    850\u001b[0m         left_indexer,\n\u001b[0;32m    851\u001b[0m         axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    852\u001b[0m         copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    853\u001b[0m         only_slice\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    854\u001b[0m         allow_dups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    855\u001b[0m         use_na_proxy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    856\u001b[0m     )\n\u001b[0;32m    857\u001b[0m     left \u001b[38;5;241m=\u001b[39m left\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(lmgr, axes\u001b[38;5;241m=\u001b[39mlmgr\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:688\u001b[0m, in \u001b[0;36mBaseBlockManager.reindex_indexer\u001b[1;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[0m\n\u001b[0;32m    686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    687\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 688\u001b[0m         blk\u001b[38;5;241m.\u001b[39mtake_nd(\n\u001b[0;32m    689\u001b[0m             indexer,\n\u001b[0;32m    690\u001b[0m             axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    691\u001b[0m             fill_value\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    692\u001b[0m                 fill_value \u001b[38;5;28;01mif\u001b[39;00m fill_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m blk\u001b[38;5;241m.\u001b[39mfill_value\n\u001b[0;32m    693\u001b[0m             ),\n\u001b[0;32m    694\u001b[0m         )\n\u001b[0;32m    695\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks\n\u001b[0;32m    696\u001b[0m     ]\n\u001b[0;32m    698\u001b[0m new_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:1307\u001b[0m, in \u001b[0;36mBlock.take_nd\u001b[1;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;66;03m# Note: algos.take_nd has upcast logic similar to coerce_to_target_dtype\u001b[39;00m\n\u001b[1;32m-> 1307\u001b[0m new_values \u001b[38;5;241m=\u001b[39m algos\u001b[38;5;241m.\u001b[39mtake_nd(\n\u001b[0;32m   1308\u001b[0m     values, indexer, axis\u001b[38;5;241m=\u001b[39maxis, allow_fill\u001b[38;5;241m=\u001b[39mallow_fill, fill_value\u001b[38;5;241m=\u001b[39mfill_value\n\u001b[0;32m   1309\u001b[0m )\n\u001b[0;32m   1311\u001b[0m \u001b[38;5;66;03m# Called from three places in managers, all of which satisfy\u001b[39;00m\n\u001b[0;32m   1312\u001b[0m \u001b[38;5;66;03m#  these assertions\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py:117\u001b[0m, in \u001b[0;36mtake_nd\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    116\u001b[0m arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(arr)\n\u001b[1;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _take_nd_ndarray(arr, indexer, axis, fill_value, allow_fill)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py:157\u001b[0m, in \u001b[0;36m_take_nd_ndarray\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 157\u001b[0m     out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(out_shape, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    159\u001b[0m func \u001b[38;5;241m=\u001b[39m _get_take_nd_function(\n\u001b[0;32m    160\u001b[0m     arr\u001b[38;5;241m.\u001b[39mndim, arr\u001b[38;5;241m.\u001b[39mdtype, out\u001b[38;5;241m.\u001b[39mdtype, axis\u001b[38;5;241m=\u001b[39maxis, mask_info\u001b[38;5;241m=\u001b[39mmask_info\n\u001b[0;32m    161\u001b[0m )\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 36.9 GiB for an array with shape (5, 989712018) and data type object",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 262\u001b[0m\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mDone. Check /mnt/data for outputs (merged_data.csv, plots, feature_importances.csv if any).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 262\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[8], line 241\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28mprint\u001b[39m(summary_df\u001b[38;5;241m.\u001b[39mto_string(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[0;32m    240\u001b[0m \u001b[38;5;66;03m# Merge\u001b[39;00m\n\u001b[1;32m--> 241\u001b[0m merged \u001b[38;5;241m=\u001b[39m attempt_merge(dfs)\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m merged \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMerge failed or produced no data.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 106\u001b[0m, in \u001b[0;36mattempt_merge\u001b[1;34m(dfs)\u001b[0m\n\u001b[0;32m    104\u001b[0m             merged[key] \u001b[38;5;241m=\u001b[39m merged[key]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m    105\u001b[0m             df[key] \u001b[38;5;241m=\u001b[39m df[key]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m--> 106\u001b[0m             merged \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(merged, df, on\u001b[38;5;241m=\u001b[39mkey, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mouter\u001b[39m\u001b[38;5;124m'\u001b[39m, suffixes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# Attach the remaining dfs sensibly\u001b[39;00m\n\u001b[0;32m    108\u001b[0m remaining \u001b[38;5;241m=\u001b[39m [n \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m dfs\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m dfs_with_key]\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:184\u001b[0m, in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    170\u001b[0m     op \u001b[38;5;241m=\u001b[39m _MergeOperation(\n\u001b[0;32m    171\u001b[0m         left_df,\n\u001b[0;32m    172\u001b[0m         right_df,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m         validate\u001b[38;5;241m=\u001b[39mvalidate,\n\u001b[0;32m    183\u001b[0m     )\n\u001b[1;32m--> 184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result(copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:886\u001b[0m, in \u001b[0;36m_MergeOperation.get_result\u001b[1;34m(self, copy)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindicator:\n\u001b[0;32m    884\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indicator_pre_merge(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright)\n\u001b[1;32m--> 886\u001b[0m join_index, left_indexer, right_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_join_info()\n\u001b[0;32m    888\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_and_concat(\n\u001b[0;32m    889\u001b[0m     join_index, left_indexer, right_indexer, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    890\u001b[0m )\n\u001b[0;32m    891\u001b[0m result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_type)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:1151\u001b[0m, in \u001b[0;36m_MergeOperation._get_join_info\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1147\u001b[0m     join_index, right_indexer, left_indexer \u001b[38;5;241m=\u001b[39m _left_join_on_index(\n\u001b[0;32m   1148\u001b[0m         right_ax, left_ax, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_join_keys, sort\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1151\u001b[0m     (left_indexer, right_indexer) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_join_indexers()\n\u001b[0;32m   1153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_index:\n\u001b[0;32m   1154\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:1125\u001b[0m, in \u001b[0;36m_MergeOperation._get_join_indexers\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;66;03m# make mypy happy\u001b[39;00m\n\u001b[0;32m   1124\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhow \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masof\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m get_join_indexers(\n\u001b[0;32m   1126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft_join_keys, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_join_keys, sort\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhow\n\u001b[0;32m   1127\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:1759\u001b[0m, in \u001b[0;36mget_join_indexers\u001b[1;34m(left_keys, right_keys, sort, how)\u001b[0m\n\u001b[0;32m   1757\u001b[0m     _, lidx, ridx \u001b[38;5;241m=\u001b[39m left\u001b[38;5;241m.\u001b[39mjoin(right, how\u001b[38;5;241m=\u001b[39mhow, return_indexers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, sort\u001b[38;5;241m=\u001b[39msort)\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1759\u001b[0m     lidx, ridx \u001b[38;5;241m=\u001b[39m get_join_indexers_non_unique(\n\u001b[0;32m   1760\u001b[0m         left\u001b[38;5;241m.\u001b[39m_values, right\u001b[38;5;241m.\u001b[39m_values, sort, how\n\u001b[0;32m   1761\u001b[0m     )\n\u001b[0;32m   1763\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lidx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m is_range_indexer(lidx, \u001b[38;5;28mlen\u001b[39m(left)):\n\u001b[0;32m   1764\u001b[0m     lidx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:1801\u001b[0m, in \u001b[0;36mget_join_indexers_non_unique\u001b[1;34m(left, right, sort, how)\u001b[0m\n\u001b[0;32m   1799\u001b[0m     lidx, ridx \u001b[38;5;241m=\u001b[39m libjoin\u001b[38;5;241m.\u001b[39minner_join(lkey, rkey, count, sort\u001b[38;5;241m=\u001b[39msort)\n\u001b[0;32m   1800\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m how \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mouter\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1801\u001b[0m     lidx, ridx \u001b[38;5;241m=\u001b[39m libjoin\u001b[38;5;241m.\u001b[39mfull_outer_join(lkey, rkey, count)\n\u001b[0;32m   1802\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m lidx, ridx\n",
      "File \u001b[1;32mjoin.pyx:189\u001b[0m, in \u001b[0;36mpandas._libs.join.full_outer_join\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 7.37 GiB for an array with shape (989712018,) and data type int64"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Modeling imports \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "DATA_DIR = r\"C:\\Users\\ASUS\\Downloads\\crop_dataset\"   \n",
    "\n",
    "def find_csv_files(data_dir):\n",
    "    files = [f for f in os.listdir(data_dir) if f.lower().endswith('.csv')]\n",
    "    files.sort()\n",
    "    return files\n",
    "\n",
    "def safe_read_csv(path):\n",
    "    try:\n",
    "        return pd.read_csv(path)\n",
    "    except Exception:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding='latin1')\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read {path}: {e}\")\n",
    "            return None\n",
    "\n",
    "def summarize_dfs(dfs):\n",
    "    summary = []\n",
    "    for name, df in dfs.items():\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"File: {name}\")\n",
    "        print(\"- shape:\", df.shape)\n",
    "        print(\"\\n-- head()\")\n",
    "        print(df.head().to_string(index=False))\n",
    "        print(\"\\n-- dtypes:\")\n",
    "        print(df.dtypes)\n",
    "        print(\"\\n-- describe():\")\n",
    "        print(df.describe().T)\n",
    "        miss = df.isnull().sum()\n",
    "        miss_pct = miss / len(df) * 100\n",
    "        miss_df = pd.concat([miss, miss_pct], axis=1)\n",
    "        miss_df.columns = ['missing_count', 'missing_pct']\n",
    "        print(\"\\n-- top missing columns:\")\n",
    "        print(miss_df.sort_values('missing_count', ascending=False).head(20))\n",
    "        summary.append({\n",
    "            'file': name,\n",
    "            'shape': df.shape,\n",
    "            'num_columns': df.shape[1],\n",
    "            'num_missing_columns': int((miss > 0).sum())\n",
    "        })\n",
    "    return pd.DataFrame(summary)\n",
    "\n",
    "def find_common_columns(dfs):\n",
    "    all_cols = []\n",
    "    for df in dfs.values():\n",
    "        all_cols.extend(list(df.columns))\n",
    "    common_counts = Counter(all_cols)\n",
    "    return [col for col, cnt in common_counts.items() if cnt > 1]\n",
    "\n",
    "def attempt_merge(dfs):\n",
    "    \n",
    "    common_cols = find_common_columns(dfs)\n",
    "    candidate_keys = [c for c in common_cols if c.lower() in ('date','state','district','location','crop','year','month')]\n",
    "    print(\"Common columns across files:\", common_cols)\n",
    "    print(\"Candidate merge keys:\", candidate_keys)\n",
    "\n",
    "    if candidate_keys:\n",
    "        key = candidate_keys[0]\n",
    "        print(\"Merging on key:\", key)\n",
    "\n",
    "        dfs_with_key = {n: df for n, df in dfs.items() if key in df.columns}\n",
    "        merged = None\n",
    "        for name, df in dfs_with_key.items():\n",
    "            if merged is None:\n",
    "                merged = df.copy()\n",
    "            else:\n",
    "                try:\n",
    "                    merged = pd.merge(merged, df, on=key, how='outer', suffixes=('', f'_{name.split('.')[0]}'))\n",
    "                except Exception:\n",
    "                   \n",
    "                    merged[key] = merged[key].astype(str)\n",
    "                    df[key] = df[key].astype(str)\n",
    "                    merged = pd.merge(merged, df, on=key, how='outer', suffixes=('', f'_{name.split('.')[0]}'))\n",
    "       \n",
    "        remaining = [n for n in dfs.keys() if n not in dfs_with_key]\n",
    "        for name in remaining:\n",
    "            df = dfs[name]\n",
    "            if merged is not None and len(df) == len(merged):\n",
    "                merged = pd.concat([merged.reset_index(drop=True), df.reset_index(drop=True)], axis=1)\n",
    "            else:\n",
    "                merged = pd.concat([merged, df], ignore_index=True, sort=False)\n",
    "    else:\n",
    "        \n",
    "        groups = {}\n",
    "        for name, df in dfs.items():\n",
    "            col_key = tuple(df.columns)\n",
    "            groups.setdefault(col_key, []).append(df)\n",
    "        if any(len(v) > 1 for v in groups.values()):\n",
    "            merged_parts = []\n",
    "            for k, v in groups.items():\n",
    "                if len(v) == 1:\n",
    "                    merged_parts.append(v[0])\n",
    "                else:\n",
    "                    merged_parts.append(pd.concat(v, ignore_index=True, sort=False))\n",
    "            merged = pd.concat(merged_parts, ignore_index=True, sort=False)\n",
    "        else:\n",
    "            merged = pd.concat(dfs.values(), ignore_index=True, sort=False)\n",
    "    return merged\n",
    "\n",
    "def save_df(df, path):\n",
    "    df.to_csv(path, index=False)\n",
    "    print(f\"Saved: {path}\")\n",
    "\n",
    "def plot_missing_values(df, outpath):\n",
    "    miss = df.isnull().sum().sort_values(ascending=False).head(20)\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.barh(miss.index[::-1], miss.values[::-1])\n",
    "    plt.title(\"Top 20 columns by missing values\")\n",
    "    plt.xlabel(\"Missing values\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outpath)\n",
    "    plt.close()\n",
    "    print(\"Saved:\", outpath)\n",
    "\n",
    "def plot_correlation_heatmap(df, outpath):\n",
    "    numeric = df.select_dtypes(include=[np.number])\n",
    "    if numeric.shape[1] < 2:\n",
    "        print(\"Not enough numeric columns for correlation heatmap.\")\n",
    "        return\n",
    "    corr = numeric.corr()\n",
    "    top_cols = corr.abs().sum().sort_values(ascending=False).head(12).index\n",
    "    M = numeric[top_cols].corr()\n",
    "    plt.figure(figsize=(8,6))\n",
    "    im = plt.imshow(M, interpolation='none', cmap='viridis')\n",
    "    plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "    plt.xticks(range(len(top_cols)), top_cols, rotation=45, ha='right')\n",
    "    plt.yticks(range(len(top_cols)), top_cols)\n",
    "    plt.title(\"Correlation heatmap (top numeric columns)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outpath)\n",
    "    plt.close()\n",
    "    print(\"Saved:\", outpath)\n",
    "\n",
    "def attempt_modeling(merged_df, data_dir):\n",
    "    yield_cols = [c for c in merged_df.columns if 'yield' in c.lower()]\n",
    "    if not yield_cols:\n",
    "        print(\"No yield-like column found, skipping modeling.\")\n",
    "        return\n",
    "    target_col = yield_cols[0]\n",
    "    print(\"Using target:\", target_col)\n",
    "    df = merged_df.copy()\n",
    "    y = df[target_col]\n",
    "    X = df.drop(columns=[target_col])\n",
    "    X_num = X.select_dtypes(include=[np.number]).copy()\n",
    "\n",
    "    if X_num.shape[1] == 0:\n",
    "        print(\"No numeric features available for modeling.\")\n",
    "        return\n",
    "    \n",
    "    thresh_missing = 0.6 * len(X_num)\n",
    "    to_keep = [c for c in X_num.columns if X_num[c].isnull().sum() <= thresh_missing]\n",
    "    X_num = X_num[to_keep]\n",
    "    \n",
    "    mask = y.notnull()\n",
    "    X_num = X_num.loc[mask]\n",
    "    y_model = y.loc[mask]\n",
    "    if X_num.shape[0] < 30:\n",
    "        print(\"Not enough rows with non-null target for reliable modeling. Rows:\", X_num.shape[0])\n",
    "        return\n",
    "    # Simple pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1))\n",
    "    ])\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(pipeline, X_num, y_model, cv=cv, scoring='r2', n_jobs=1)\n",
    "    print(\"Cross-validated R2 scores:\", scores)\n",
    "    print(\"Mean R2:\", scores.mean())\n",
    "    # Fit and get feature importances\n",
    "    pipeline.fit(X_num, y_model)\n",
    "    model = pipeline.named_steps['model']\n",
    "    try:\n",
    "        importances = pd.Series(model.feature_importances_, index=X_num.columns).sort_values(ascending=False)\n",
    "        fi_path = os.path.join(data_dir, 'feature_importances.csv')\n",
    "        importances.to_csv(fi_path, header=['importance'])\n",
    "        print(\"Saved feature importances to:\", fi_path)\n",
    "        print(importances.head(10).to_string())\n",
    "    except Exception as e:\n",
    "        print(\"Model does not support feature_importances_ or failed to extract:\", e)\n",
    "\n",
    "def main():\n",
    "    csv_files = find_csv_files(DATA_DIR)\n",
    "    if not csv_files:\n",
    "        print(\"No CSV files found in\", DATA_DIR)\n",
    "        return\n",
    "    print(\"Found CSV files:\", csv_files)\n",
    "    dfs = {}\n",
    "    for f in csv_files:\n",
    "        path = os.path.join(DATA_DIR, f)\n",
    "        df = safe_read_csv(path)\n",
    "        if df is not None:\n",
    "            dfs[f] = df\n",
    "        else:\n",
    "            print(\"Skipping file (could not read):\", f)\n",
    "\n",
    "    # Summarize\n",
    "    summary_df = summarize_dfs(dfs)\n",
    "    print(\"\\nSummary table:\")\n",
    "    print(summary_df.to_string(index=False))\n",
    "\n",
    "    # Merge\n",
    "    merged = attempt_merge(dfs)\n",
    "    if merged is None:\n",
    "        print(\"Merge failed or produced no data.\")\n",
    "        return\n",
    "    print(\"\\nMerged shape:\", merged.shape)\n",
    "    print(merged.head().to_string(index=False))\n",
    "\n",
    "    # Save merged\n",
    "    merged_path = os.path.join(DATA_DIR, 'merged_data.csv')\n",
    "    save_df(merged, merged_path)\n",
    "\n",
    "    # EDA plots\n",
    "    plot_missing_values(merged, os.path.join(DATA_DIR, 'missing_values_top20.png'))\n",
    "    plot_correlation_heatmap(merged, os.path.join(DATA_DIR, 'correlation_heatmap.png'))\n",
    "\n",
    "    # Attempt modeling\n",
    "    attempt_modeling(merged, DATA_DIR)\n",
    "\n",
    "    print(\"\\nDone. Check /mnt/data for outputs (merged_data.csv, plots, feature_importances.csv if any).\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
